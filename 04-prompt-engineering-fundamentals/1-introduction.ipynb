{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was auto-generated by GitHub Copilot Chat and is meant for initial setup only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering using the OpenAI models for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[198, 262, 50789, 374, 279, 18172, 11841, 505, 279, 8219, 323, 279, 257, 7928, 304, 279, 25450, 744, 13, 1102, 374, 264, 6962, 14880, 449, 257, 264, 3148, 832, 7716, 52949, 339, 430, 315, 279, 8219, 11, 719, 1403, 9976, 7561, 34902, 257, 3115, 430, 315, 682, 279, 1023, 33975, 304, 279, 25450, 744, 11093, 13, 257, 50789, 374, 832, 315, 279, 72021, 6302, 9621, 311, 279, 19557, 8071, 257, 304, 279, 3814, 13180, 11, 323, 706, 1027, 3967, 311, 14154, 86569, 2533, 257, 1603, 12715, 3925, 13, 1102, 374, 7086, 1306, 279, 13041, 10087, 50789, 8032, 777, 60, 257, 3277, 19894, 505, 9420, 11, 50789, 649, 387, 10107, 3403, 369, 1202, 27000, 257, 3177, 311, 6445, 9621, 35612, 17706, 508, 60, 323, 374, 389, 5578, 279, 4948, 1481, 1315, 478, 257, 5933, 1665, 304, 279, 3814, 13180, 1306, 279, 17781, 323, 50076, 627, 257]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'\\n',\n",
       " b'   ',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' the',\n",
       " b' fifth',\n",
       " b' planet',\n",
       " b' from',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b' and',\n",
       " b' the',\n",
       " b'    ',\n",
       " b' largest',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' a',\n",
       " b' gas',\n",
       " b' giant',\n",
       " b' with',\n",
       " b'    ',\n",
       " b' a',\n",
       " b' mass',\n",
       " b' one',\n",
       " b'-th',\n",
       " b'ousand',\n",
       " b'th',\n",
       " b' that',\n",
       " b' of',\n",
       " b' the',\n",
       " b' Sun',\n",
       " b',',\n",
       " b' but',\n",
       " b' two',\n",
       " b'-and',\n",
       " b'-a',\n",
       " b'-half',\n",
       " b'    ',\n",
       " b' times',\n",
       " b' that',\n",
       " b' of',\n",
       " b' all',\n",
       " b' the',\n",
       " b' other',\n",
       " b' planets',\n",
       " b' in',\n",
       " b' the',\n",
       " b' Solar',\n",
       " b' System',\n",
       " b' combined',\n",
       " b'.',\n",
       " b'    ',\n",
       " b' Jupiter',\n",
       " b' is',\n",
       " b' one',\n",
       " b' of',\n",
       " b' the',\n",
       " b' brightest',\n",
       " b' objects',\n",
       " b' visible',\n",
       " b' to',\n",
       " b' the',\n",
       " b' naked',\n",
       " b' eye',\n",
       " b'    ',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b',',\n",
       " b' and',\n",
       " b' has',\n",
       " b' been',\n",
       " b' known',\n",
       " b' to',\n",
       " b' ancient',\n",
       " b' civilizations',\n",
       " b' since',\n",
       " b'    ',\n",
       " b' before',\n",
       " b' recorded',\n",
       " b' history',\n",
       " b'.',\n",
       " b' It',\n",
       " b' is',\n",
       " b' named',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Roman',\n",
       " b' god',\n",
       " b' Jupiter',\n",
       " b'.[',\n",
       " b'19',\n",
       " b']',\n",
       " b'    ',\n",
       " b' When',\n",
       " b' viewed',\n",
       " b' from',\n",
       " b' Earth',\n",
       " b',',\n",
       " b' Jupiter',\n",
       " b' can',\n",
       " b' be',\n",
       " b' bright',\n",
       " b' enough',\n",
       " b' for',\n",
       " b' its',\n",
       " b' reflected',\n",
       " b'    ',\n",
       " b' light',\n",
       " b' to',\n",
       " b' cast',\n",
       " b' visible',\n",
       " b' shadows',\n",
       " b',[',\n",
       " b'20',\n",
       " b']',\n",
       " b' and',\n",
       " b' is',\n",
       " b' on',\n",
       " b' average',\n",
       " b' the',\n",
       " b' third',\n",
       " b'-b',\n",
       " b'right',\n",
       " b'est',\n",
       " b'    ',\n",
       " b' natural',\n",
       " b' object',\n",
       " b' in',\n",
       " b' the',\n",
       " b' night',\n",
       " b' sky',\n",
       " b' after',\n",
       " b' the',\n",
       " b' Moon',\n",
       " b' and',\n",
       " b' Venus',\n",
       " b'.\\n',\n",
       " b'    ']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "    Jupiter is the fifth planet from the Sun and the \\\n",
    "    largest in the Solar System. It is a gas giant with \\\n",
    "    a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "    times that of all the other planets in the Solar System combined. \\\n",
    "    Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "    in the night sky, and has been known to ancient civilizations since \\\n",
    "    before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "    When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "    light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "    natural object in the night sky after the Moon and Venus.\n",
    "    \"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens)\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted\n",
    ")\n",
    "\n",
    "## Updated\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m```\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m## 3. Run the prompt\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m, in \u001b[0;36mget_completion\u001b[1;34m(prompt, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_completion\u001b[39m(prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     15\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m---> 16\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this is the degree of randomness of the model's output\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1043\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1051\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1052\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1053\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1054\u001b[0m     )\n\u001b[1;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    832\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    833\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:865\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m--> 865\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:925\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 877\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "## ---------- Call the helper method\n",
    "\n",
    "### 1. Set primary content or prompt text\n",
    "text = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Use that in the prompt template below\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## 3. Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Martian War of 2076 - A Lesson Plan\n",
      "\n",
      "Objective: \n",
      "To educate students about the Martian War of 2076, its causes, key events, and consequences, fostering critical thinking, historical analysis, and empathy.\n",
      "\n",
      "Grade Level: \n",
      "High School (9th-12th grade)\n",
      "\n",
      "Duration: \n",
      "2-3 class periods (approximately 90 minutes each)\n",
      "\n",
      "Materials Needed:\n",
      "1. Access to research materials (books, articles, online resources)\n",
      "2. Whiteboard or blackboard\n",
      "3. Markers or chalk\n",
      "4. Handouts (optional)\n",
      "5. Multimedia resources (optional)\n",
      "\n",
      "Lesson Plan:\n",
      "\n",
      "Day 1:\n",
      "\n",
      "Introduction (15 minutes)\n",
      "1. Begin the lesson by engaging students in a brief discussion about science fiction and its influence on society.\n",
      "2. Introduce the topic of the Martian War of 2076, explaining that it is a fictional event but will be studied as if it were real.\n",
      "3. Share the lesson objectives and explain the importance of understanding historical events, even if they are fictional.\n",
      "\n",
      "Causes of the Martian War (30 minutes)\n",
      "1. Divide students into small groups and provide each group with research materials.\n",
      "2. Instruct students to identify and discuss the possible causes of the Martian War of 2076.\n",
      "3. After group discussions, facilitate a class discussion, allowing each group to share their findings.\n",
      "4. Summarize the causes on the board, encouraging students to critically analyze the factors that led to the war.\n",
      "\n",
      "Key Events of the Martian War (45 minutes)\n",
      "1. Provide students with a timeline of the Martian War, highlighting key events.\n",
      "2. Instruct students to work individually or in pairs to research and summarize each event.\n",
      "3. Allow students time to present their findings to the class, discussing the significance of each event.\n",
      "4. Facilitate a class discussion to analyze the sequence of events and their impact on the war's outcome.\n",
      "\n",
      "Day 2:\n",
      "\n",
      "Consequences of the Martian War (30 minutes)\n",
      "1. Discuss with students the consequences of the Martian War, both for Earth and Mars.\n",
      "2. Divide students into small groups and assign each group a specific consequence to research.\n",
      "3. Instruct students to prepare a short presentation highlighting the consequences and their long-term effects.\n",
      "4. Allow each group to present their findings to the class, encouraging critical thinking and discussion.\n",
      "\n",
      "Analyzing Perspectives (45 minutes)\n",
      "1. Divide students into pairs or small groups.\n",
      "2. Assign each group a specific perspective related to the Martian War (e.g., Earth government, Martian rebels, civilians, scientists).\n",
      "3. Instruct students to research and analyze the motivations, actions, and perspectives of their assigned group.\n",
      "4. Encourage students to consider the ethical dilemmas faced by each group and the impact on their decision-making.\n",
      "5. Facilitate a class discussion, allowing each group to share their findings and promoting empathy and understanding.\n",
      "\n",
      "Reflection and Discussion (15 minutes)\n",
      "1. Conclude the lesson by asking students to reflect on the Martian War of 2076 and its relevance to real-world conflicts.\n",
      "2. Engage students in a class discussion, encouraging them to share their thoughts, opinions, and lessons learned.\n",
      "3. Summarize the key takeaways from the lesson, emphasizing the importance of critical thinking, empathy, and historical analysis.\n",
      "\n",
      "Optional Extension Activities:\n",
      "1. Creative Writing: Ask students to write a short story or narrative set during the Martian War, incorporating historical facts and fictional elements.\n",
      "2. Debate: Organize a class debate where students take on different roles and argue for or against the Martian War, exploring different perspectives.\n",
      "3. Multimedia Presentation: Allow students to create multimedia presentations (videos, slideshows) summarizing the Martian War and its impact.\n",
      "\n",
      "Note: While the Martian War of 2076 is a fictional event, it is essential to emphasize the importance of critical thinking and historical analysis, even when studying fictional scenarios.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupiter is a really big planet that is fifth from the Sun. It is made of gas and is the largest planet in our Solar System. It is much smaller than the Sun, but much bigger than all the other planets combined. People have known about Jupiter for a really long time because it is very bright in the night sky. It is named after a god from ancient Rome. Sometimes, Jupiter is so bright that it can make shadows on Earth. It is usually the third-brightest thing we can see at night, after the Moon and Venus.\n"
     ]
    }
   ],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, since you're so curious, the World Series in 2020 was played entirely in Arlington, Texas at the Globe Life Field. Now you can impress all your friends with that useless tidbit of information.\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module openai.resources.chat.completions:\n",
      "\n",
      "create(*, messages: 'List[ChatCompletionMessageParam]', model: \"Union[str, Literal['gpt-4-1106-preview', 'gpt-4-vision-preview', 'gpt-4', 'gpt-4-0314', 'gpt-4-0613', 'gpt-4-32k', 'gpt-4-32k-0314', 'gpt-4-32k-0613', 'gpt-3.5-turbo-1106', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k-0613']]\", frequency_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, function_call: 'completion_create_params.FunctionCall | NotGiven' = NOT_GIVEN, functions: 'List[completion_create_params.Function] | NotGiven' = NOT_GIVEN, logit_bias: 'Optional[Dict[str, int]] | NotGiven' = NOT_GIVEN, max_tokens: 'Optional[int] | NotGiven' = NOT_GIVEN, n: 'Optional[int] | NotGiven' = NOT_GIVEN, presence_penalty: 'Optional[float] | NotGiven' = NOT_GIVEN, response_format: 'completion_create_params.ResponseFormat | NotGiven' = NOT_GIVEN, seed: 'Optional[int] | NotGiven' = NOT_GIVEN, stop: 'Union[Optional[str], List[str]] | NotGiven' = NOT_GIVEN, stream: 'Optional[Literal[False]] | Literal[True] | NotGiven' = NOT_GIVEN, temperature: 'Optional[float] | NotGiven' = NOT_GIVEN, tool_choice: 'ChatCompletionToolChoiceOptionParam | NotGiven' = NOT_GIVEN, tools: 'List[ChatCompletionToolParam] | NotGiven' = NOT_GIVEN, top_p: 'Optional[float] | NotGiven' = NOT_GIVEN, user: 'str | NotGiven' = NOT_GIVEN, extra_headers: 'Headers | None' = None, extra_query: 'Query | None' = None, extra_body: 'Body | None' = None, timeout: 'float | httpx.Timeout | None | NotGiven' = NOT_GIVEN) -> 'ChatCompletion | Stream[ChatCompletionChunk]' method of openai.resources.chat.completions.Completions instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(openai.chat.completions.create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le président actuel du Togo est Faure Gnassingbé.\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Qui est le président actuel du Togo?\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...l'homme aiguise l'esprit de l'homme. Cette expression fait référence à l'idée que les interactions entre les individus peuvent les aider à se développer et à s'améliorer mutuellement. De la même manière que le fer peut être affûté en frottant deux morceaux ensemble, les personnes peuvent se stimuler intellectuellement et émotionnellement en se confrontant les unes aux autres. Cela peut se produire dans des discussions, des débats ou des collaborations, où les idées et les perspectives sont échangées et affinées. Cette expression souligne l'importance des relations interpersonnelles dans le développement personnel et la croissance intellectuelle.\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Comme le fer aiguise le fer...\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fabrication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre du cours : La 4e Guerre mondiale en 2075 : Analyse et conséquences\n",
      "\n",
      "I. Introduction\n",
      "   A. Présentation du contexte mondial en 2075\n",
      "   B. Définition de la 4e Guerre mondiale\n",
      "   C. Objectifs du cours\n",
      "\n",
      "II. Causes et déclenchement de la 4e Guerre mondiale\n",
      "   A. Analyse des tensions géopolitiques préexistantes\n",
      "   B. Étude des facteurs déclencheurs\n",
      "   C. Rôle des avancées technologiques dans le déclenchement du conflit\n",
      "\n",
      "III. Acteurs et alliances\n",
      "   A. Présentation des principales puissances impliquées\n",
      "   B. Analyse des alliances et des coalitions formées\n",
      "   C. Étude des motivations et des intérêts des différents acteurs\n",
      "\n",
      "IV. Déroulement de la guerre\n",
      "   A. Chronologie des événements majeurs\n",
      "   B. Présentation des stratégies militaires utilisées\n",
      "   C. Impact des nouvelles technologies sur les tactiques de combat\n",
      "\n",
      "V. Conséquences de la 4e Guerre mondiale\n",
      "   A. Bilan humain et matériel\n",
      "   B. Conséquences économiques et environnementales\n",
      "   C. Réorganisation géopolitique mondiale\n",
      "\n",
      "VI. Leçons tirées de la 4e Guerre mondiale\n",
      "   A. Analyse des erreurs commises et des succès obtenus\n",
      "   B. Réflexion sur les enjeux de la paix et de la sécurité mondiale\n",
      "   C. Propositions pour prévenir de futurs conflits mondiaux\n",
      "\n",
      "VII. Conclusion\n",
      "   A. Récapitulation des points clés abordés dans le cours\n",
      "   B. Réflexion sur l'importance de l'étude de l'histoire pour éviter les erreurs du passé\n",
      "   C. Ouverture sur les perspectives d'avenir\n",
      "\n",
      "Note : Ce plan est une suggestion et peut être adapté en fonction des besoins spécifiques du cours et du niveau des étudiants.\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Faire un plan pour un cours sur la 4e guerre mondiale en 2075\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Le vitiligo est une maladie chronique de la peau qui se caractérise par des taches blanches sur la peau et une dépigmentation des poils. Ces taches peuvent varier en taille, apparence et emplacement, et peuvent être plus ou moins évolutives et généralisées.\n",
      "\n",
      "2. Le vitiligo est une maladie auto-immune acquise, ce qui signifie que le système immunitaire attaque les mélanocytes, les cellules responsables de la production de pigment dans la peau. Les mélanocytes meurent ou ne fonctionnent pas normalement, ce qui entraîne la perte de pigmentation.\n",
      "\n",
      "3. Les causes exactes du vitiligo ne sont pas bien connues, mais des recherches suggèrent qu'il peut être causé par des troubles auto-immuns, des prédispositions génétiques, des stress oxydatifs, des facteurs neuronaux ou viraux. Des facteurs tels que le stress, l'anxiété, les chocs psychologiques et les frottements peuvent également influencer l'évolution de la maladie.\n"
     ]
    }
   ],
   "source": [
    "## Example text\n",
    "text = f\"\"\"\n",
    "Le vitiligo — aussi appelé leucodermie ou achromie —, peut-être du latin vitium (« défaut, vice ») ou vitelius (« viande blanche des veaux »), \n",
    "est une maladie chronique de l'épiderme. Ce trouble pauci-symptomatique se caractérise par des taches blanches (leucodermie) sur la peau\n",
    "et parfois par une dépigmentation des poils (leucotrichie). Les macules blancs ou taches circonscrites sur la peau, de dimension,\n",
    "d'aspect et de localisation variables, sont plus ou moins évolutives, et plus ou moins généralisées.\n",
    "Cette maladie auto-immune acquise survient lorsque les mélanocytes meurent ou sont incapables de fonctionner normalement.\n",
    "Elle évolue par poussées, sous l'influence de différents facteurs comme le stress, l'anxiété, le choc psychologique ou les frottements,\n",
    "et apparaît parfois au contact de produits chimiques (rares cas). Les causes du vitiligo sont peu connues.\n",
    "Des recherches suggèrent cependant que la maladie puisse provenir de désordres auto-immuns, de prédispositions génétiques, de stress oxydatifs.\n",
    "Le vitiligo peut par ailleurs être d'origine neuronale ou virale.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Explique ce texte en 3 points\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
